{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import sqrt, e, log\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_bound(rho, d, m, delta):\n",
    "    \"\"\"\n",
    "    Calculate the error bound\n",
    "    Args:\n",
    "        rho:    margin\n",
    "        d:      VC dimension\n",
    "        m:      sample size\n",
    "        delta:  confidence\n",
    "    Output:\n",
    "        error:  theoretical error bound\n",
    "    \"\"\"\n",
    "    first_term = sqrt((2 * d * log(e * m / d)) / m) * 2 / rho\n",
    "    second_term = sqrt(log(1 / delta) / (2 * m))\n",
    "    return first_term + second_term\n",
    "\n",
    "\n",
    "def create_dataset(m, d, class_sep=1, random_state=42):\n",
    "    \"\"\"\n",
    "    Create a dataset for the experiment\n",
    "    Args:\n",
    "        m:              sample size\n",
    "        d:              VC dimension\n",
    "        class_sep:      class separation\n",
    "        random_state:   random seed\n",
    "    Output:\n",
    "        X_train:    training data\n",
    "        X_test:     test data\n",
    "        y_train:    training labels\n",
    "        y_test:     test labels\n",
    "    \"\"\"\n",
    "    X, y = make_classification(\n",
    "        n_samples=2*m,\n",
    "        n_features=d-1,\n",
    "        n_classes=2,\n",
    "        random_state=random_state,\n",
    "        shuffle=False,\n",
    "        class_sep=class_sep,\n",
    "        flip_y=0.05,\n",
    "    )\n",
    "    # Changle class labels to +1 and -1\n",
    "    y = 2*y - 1\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def evaluate_error(X_train, X_test, y_train, y_test, T=50):\n",
    "    \"\"\"\n",
    "    Evaluate the error of the AdaBoost classifier\n",
    "    Args:\n",
    "        X_train:    training data\n",
    "        X_test:     test data\n",
    "        y_train:    training labels\n",
    "        y_test:     test labels\n",
    "        T:          number of iterations\n",
    "    Output:\n",
    "        R_train:    training error\n",
    "        R_test:     test error\n",
    "    \"\"\"\n",
    "    # Train and fit AdaBoost with perceptron as base estimator\n",
    "    clf = AdaBoostClassifier(\n",
    "        base_estimator=Perceptron(penalty=None),\n",
    "        n_estimators=T,\n",
    "        algorithm='SAMME',\n",
    "        random_state=42,\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Calculate the training and test error\n",
    "    R_train = 1 - clf.score(X_train, y_train)\n",
    "    R_test  = 1 - clf.score(X_test, y_test)\n",
    "\n",
    "    return R_train, R_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value m = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value m = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value m = 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value m = 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix the margin and confidence parameter\n",
    "rho = 15\n",
    "delta = 0.05\n",
    "\n",
    "# Code for the experiment 3.3\n",
    "experiment_d, error_bounds_d = {}, {}\n",
    "range_d = range(10, 1000, 100)\n",
    "for m in [500, 1000, 1500, 2000]:\n",
    "    true_error, bound = [], []\n",
    "    for d in tqdm(range_d):\n",
    "        X_train, X_test, y_train, y_test = create_dataset(m, d)\n",
    "        R_train, R_test = evaluate_error(X_train, X_test, y_train, y_test)\n",
    "        bound.append(get_error_bound(rho, d, m, delta))\n",
    "        true_error.append(R_test - R_train)\n",
    "    experiment_d[m] = true_error\n",
    "    error_bounds_d[m] = bound\n",
    "    print(f'Finished for the value m = {m}')\n",
    "\n",
    "df_experiment_d = pd.DataFrame(experiment_d)\n",
    "df_experiment_d['d'] = range_d\n",
    "# df_experiment_d.to_csv('data/experiment_d.csv', index=False)\n",
    "\n",
    "df_error_bounds_d = pd.DataFrame(error_bounds_d)\n",
    "df_error_bounds_d['d'] = range_d\n",
    "# df_error_bounds_d.to_csv('data/bounds_d.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 49.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value d = 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 39.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value d = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 30.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value d = 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 14.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished for the value d = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix the margin and confidence parameter\n",
    "rho = 15\n",
    "delta = 0.05\n",
    "\n",
    "# Code for the experiment 3.2\n",
    "experiment_m, error_bounds_m = {}, {}\n",
    "range_m = range(100, 10000, 10)\n",
    "for d in [25, 50, 75, 100]:\n",
    "    true_error, bound = [], []\n",
    "    for m in tqdm(range_m):\n",
    "        X_train, X_test, y_train, y_test = create_dataset(m, d)\n",
    "        R_train, R_test = evaluate_error(X_train, X_test, y_train, y_test)\n",
    "        bound.append(get_error_bound(rho, d, m, delta))\n",
    "        true_error.append(R_test - R_train)\n",
    "    experiment_m[d] = true_error\n",
    "    error_bounds_m[d] = bound\n",
    "    print(f'Finished for the value d = {d}')\n",
    "\n",
    "df_error_bounds_m = pd.DataFrame(error_bounds_m)\n",
    "df_error_bounds_m['m'] = range_m\n",
    "# df_error_bounds_m.to_csv('data/bounds_m.csv', index=False)\n",
    "\n",
    "df_experiment_m = pd.DataFrame(experiment_m)\n",
    "df_experiment_m['m'] = range_m\n",
    "# df_experiment_m.to_csv('data/experiment_m.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:40<00:00,  1.01s/it]\n",
      "100%|██████████| 100/100 [01:43<00:00,  1.03s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fix the confidence parameter, VS-dimension and number of samples\n",
    "delta = 0.05\n",
    "d = 100\n",
    "m = 500\n",
    "\n",
    "# Code for the experiment 3.1\n",
    "for i, (d, m) in enumerate([(100, 500), (50, 1000)]):\n",
    "    range_t = range(1, 30)\n",
    "    test_errors, train_errors = [0] * 30, [0] * 30\n",
    "    n_iter = 100\n",
    "    for j in tqdm(range(n_iter)):\n",
    "        X_train, X_test, y_train, y_test = create_dataset(m, d, class_sep=0.5, random_state=j+1337)\n",
    "        for T in range_t:\n",
    "            R_test, R_train = evaluate_error(X_train, X_test, y_train, y_test, T)\n",
    "            test_errors[T] += R_test\n",
    "            train_errors[T] += R_train\n",
    "    test_errors = [e / n_iter for e in test_errors[1:]]\n",
    "    train_errors = [e / n_iter for e in train_errors[1:]]\n",
    "    df_number_of_trees = pd.DataFrame({\n",
    "        'T': range_t,\n",
    "        'test_error': test_errors,\n",
    "        'train_error': train_errors\n",
    "    }).to_csv(f'data/experiment_T{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'500': 82.513, '1000': 99.397, '1500': 100.0, '2000': 100.0}\n",
      "{'25': 100.0, '50': 99.9, '75': 99.7, '100': 98.99900000000001}\n"
     ]
    }
   ],
   "source": [
    "# Code to evaluate the experimental confidence parameter\n",
    "\n",
    "df_error_bounds_d = pd.read_csv('data/bounds_d.csv')\n",
    "df_experiment_d   = pd.read_csv('data/experiment_d.csv')\n",
    "df_error_bounds_m = pd.read_csv('data/bounds_m.csv')\n",
    "df_experiment_m   = pd.read_csv('data/experiment_m.csv')\n",
    "\n",
    "delta_d = {}\n",
    "for m in ['500', '1000', '1500', '2000']:\n",
    "    df_trues_d = df_error_bounds_d[m] > df_experiment_d[m]\n",
    "    delta_d[m] = round(df_trues_d.sum() / len(df_trues_d), 5) * 100\n",
    "\n",
    "delta_m = {}\n",
    "for d in ['25', '50', '75', '100']:\n",
    "    df_trues_m = df_error_bounds_m[d] > df_experiment_m[d]\n",
    "    delta_m[d] = round(df_trues_m.sum() / len(df_trues_m), 5) * 100\n",
    "\n",
    "print(delta_d)\n",
    "print(delta_m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fe40b7a1c6001628267b17b49c7a3b91f89c63956810a1e7a8974171e1f2d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
